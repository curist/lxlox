let lib = import "src/lib.lx"
let types = import "src/types.lx"
let initScanner = import "src/scanner.lx"

let TOKEN = types.TOKEN
let OP = types.OP

let mod = {}

let PREC = {
  let iota = lib.iota(0)
  {
    NONE: iota(),
    ASSIGNMENT: iota(),  // =
    OR: iota(),          // or
    AND: iota(),         // and
    EQUALITY: iota(),    // == !=
    COMPARISON: iota(),  // < > <= >=
    TERM: iota(),        // + -
    FACTOR: iota(),      // * /
    UNARY: iota(),       // ! -
    CALL: iota(),        // . ()
    PRIMARY: iota(),
    // XXX: what about [] index access pattern?
    // how do we resolve conflict between hashmap & block?
  }
}

mod.debugPrint = fn(src) {
  let scanner = initScanner(src)

  let line = -1

  for true {
    let token = scanner.scanToken()

    let lineInfo = if token.line != line {
      lib.padStart(token.line, 4, " ")
      //^ XXX: ah... that's from parent global scope :o
      // not sure if this is a behavior we want...
    } else {
      "   | "
    }
    pp([
      lineInfo,
      " ", token.type, " ",
      lib.padRight(token.lexeme, 30, " "),
      "\t| ",
      types.TOKEN_NAME[token.type],
    ])

    if token.type == types.TOKEN.EOF {
      break
    }
  }
}

mod.compile = fn(src) {
  let scanner = initScanner(src)

  // XXX: future
  // TODO: put line info as debug info, as part of the bin format
  let line = -1

  // to get around mutual references
  let parseRules
  let getRule

  let chunks = [{
    bytecode: []
    constants: []
  }]

  let parser = {
    current: 0,
    previous: 0,
    hadError: false,
    panicMode: false,
  }

  fn currentChunk() {
    // XXX: probably will need to change this later
    // Right now, the chunk pointer is stored in a module-level variable,
    // like we store other global state.
    // Later, when we start compiling user-defined functions,
    // the notion of “current chunk” gets more complicated.
    // To avoid having to go back and change a lot of code,
    // I encapsulate that logic in the currentChunk() function.
    // ^ from book text
    // https://craftinginterpreters.com/compiling-expressions.html#emitting-bytecode
    chunks[len(chunks) - 1]
  }

  fn emitByte(byte) {
    let chunk = currentChunk()
    chunk.bytecode = append(chunk.bytecode, lib.tohex(byte))
  }

  fn emitBytes(byte1, byte2) {
    emitByte(byte1)
    emitByte(byte2)
  }

  fn addConstant(chunk, value) {
    // TODO: support more value type
    chunk.constants = append(chunk.constants, value)
    return len(chunk.constants) - 1
  }

  fn makeConstant(value) {
    // TODO: support more value type
    let constant = addConstant(currentChunk(), value)
    if constant > 256 {
      error("Too many constants in one chunk.")
      return 0
    }
    return constant
  }

  fn emitConstant(value) {
    emitBytes(OP.CONSTANT, makeConstant(value))
  }

  fn errorAt(token, message) {
    if parser.panicMode {
      return
    }
    parser.panicMode = true
    pp([
      "[line ", token.line, "]",
      if token.type == TOKEN.EOF {
        " at end"
      } else if token.type == TOKEN.ERROR {
        ""
      } else {
        " at '" + token.lexeme + "'"
      },
      ": ", message,
    ])
    parser.hadError = true
  }

  fn error(message) {
    errorAt(parser.previous, message)
  }

  fn errorAtCurrent(message) {
    errorAt(parser.current, message)
  }

  fn advance() {
    parser.previous = parser.current
    for true {
      parser.current = scanner.scanToken()
      if parser.current.type != TOKEN.ERROR {
        break
      }
      errorAtCurrent(parser.current.lexeme)
    }
  }

  fn consume(type, message) {
    if parser.current.type == type {
      advance()
      return
    }
    errorAtCurrent(message)
  }

  fn emitReturn() {
    emitByte(OP.RETURN)
  }

  fn parsePrecedence(precedence) {
    advance()
    let prefixRule = getRule(parser.previous.type).prefix
    if !prefixRule {
      error("Expect expression.")
      return
    }
    prefixRule()

    for precedence <= getRule(parser.current.type).precedence {
      advance()
      let infixRule = getRule(parser.previous.type).infix
      // XXX: should we really error out here?
      if !infixRule {
        error("Expect expression.")
        return
      }
      infixRule()
    }
  }

  fn endCompiler() {
    emitReturn()
  }

  fn binary() {
    let operatorType = parser.previous.type
    let rule = getRule(operatorType)
    parsePrecedence(rule.precedence + 1)

    let opCodes = {
      [TOKEN.PLUS]: OP.ADD,
      [TOKEN.MINUS]: OP.SUBTRACT,
      [TOKEN.STAR]: OP.MULTIPLY,
      [TOKEN.SLASH]: OP.DIVIDE,
      [TOKEN.MOD]: OP.MOD,
    }
    let opCode = opCodes[operatorType]
    if opCode {
      emitByte(opCode)
    } else {
      // unreable, supposely
      error("bad binary op: " + types.TOKEN_NAME[operatorType])
    }
  }

  fn literal() {
    let operatorType = parser.previous.type

    let opCodes = {
      [TOKEN.FALSE]: OP.FALSE,
      [TOKEN.NIL]: OP.NIL,
      [TOKEN.TRUE]: OP.TRUE,
    }
    let opCode = opCodes[operatorType]
    if opCode {
      emitByte(opCode)
    } else {
      // unreable, supposely
      error("bad literal op: " + types.TOKEN_NAME[operatorType])
    }
  }

  fn expression() {
    parsePrecedence(PREC.ASSIGNMENT)
  }

  fn grouping() {
    expression()
    consume(TOKEN.RIGHT_PAREN, "Expect ')' after expression.")
  }


  fn number() {
    let value = parser.previous.literal
    if value >= 0 and value < 256 {
      emitBytes(OP.CONST_BYTE, value)
    } else {
      emitConstant(value)
    }
  }

  fn unary() {
    let opType = parser.previous.type

    parsePrecedence(PREC.UNARY)

    let opCodes = {
      [TOKEN.MINUS]: OP.NEGATE,
    }
    let opCode = opCodes[opType]
    if opCode {
      emitByte(opCode)
    } else {
      // unreable, supposely
      error("bad unary op: " + types.TOKEN_NAME[opType])
    }
  }

  parseRules = {
    [TOKEN.LEFT_PAREN]    : [grouping, nil,    PREC.NONE],
    [TOKEN.RIGHT_PAREN]   : [nil,      nil,    PREC.NONE],
    [TOKEN.LEFT_BRACE]    : [nil,      nil,    PREC.NONE], 
    [TOKEN.RIGHT_BRACE]   : [nil,      nil,    PREC.NONE],
    [TOKEN.COMMA]         : [nil,      nil,    PREC.NONE],
    [TOKEN.DOT]           : [nil,      nil,    PREC.NONE],
    [TOKEN.MINUS]         : [unary,    binary, PREC.TERM],
    [TOKEN.PLUS]          : [nil,      binary, PREC.TERM],
    [TOKEN.SEMICOLON]     : [nil,      nil,    PREC.NONE],
    [TOKEN.SLASH]         : [nil,      binary, PREC.FACTOR],
    [TOKEN.STAR]          : [nil,      binary, PREC.FACTOR],
    [TOKEN.MOD]           : [nil,      binary, PREC.FACTOR],
    [TOKEN.BANG]          : [nil,      nil,    PREC.NONE],
    [TOKEN.BANG_EQUAL]    : [nil,      nil,    PREC.NONE],
    [TOKEN.EQUAL]         : [nil,      nil,    PREC.NONE],
    [TOKEN.EQUAL_EQUAL]   : [nil,      nil,    PREC.NONE],
    [TOKEN.GREATER]       : [nil,      nil,    PREC.NONE],
    [TOKEN.GREATER_EQUAL] : [nil,      nil,    PREC.NONE],
    [TOKEN.LESS]          : [nil,      nil,    PREC.NONE],
    [TOKEN.LESS_EQUAL]    : [nil,      nil,    PREC.NONE],
    [TOKEN.IDENTIFIER]    : [nil,      nil,    PREC.NONE],
    [TOKEN.STRING]        : [nil,      nil,    PREC.NONE],
    [TOKEN.NUMBER]        : [number    nil,    PREC.NONE],
    [TOKEN.AND]           : [nil,      nil,    PREC.NONE],
    [TOKEN.CLASS]         : [nil,      nil,    PREC.NONE],
    [TOKEN.ELSE]          : [nil,      nil,    PREC.NONE],
    [TOKEN.FALSE]         : [literal,  nil,    PREC.NONE],
    [TOKEN.FOR]           : [nil,      nil,    PREC.NONE],
    [TOKEN.FN]            : [nil,      nil,    PREC.NONE],
    [TOKEN.IF]            : [nil,      nil,    PREC.NONE],
    [TOKEN.NIL]           : [literal,  nil,    PREC.NONE],
    [TOKEN.OR]            : [nil,      nil,    PREC.NONE],
    [TOKEN.PRINT]         : [nil,      nil,    PREC.NONE],
    [TOKEN.DEFER]         : [nil,      nil,    PREC.NONE],
    [TOKEN.IMPORT]        : [nil,      nil,    PREC.NONE],
    [TOKEN.RETURN]        : [nil,      nil,    PREC.NONE],
    [TOKEN.BREAK]         : [nil,      nil,    PREC.NONE],
    [TOKEN.TRUE]          : [literal,  nil,    PREC.NONE],
    [TOKEN.LET]           : [nil,      nil,    PREC.NONE],
    [TOKEN.FOR]           : [nil,      nil,    PREC.NONE],
    [TOKEN.ERROR]         : [nil,      nil,    PREC.NONE],
    [TOKEN.EOF]           : [nil,      nil,    PREC.NONE],
  }

  getRule = fn(type) {
    let rule = parseRules[type]
    return {
      prefix: rule[0],
      infix: rule[1],
      precedence: rule[2],
    }
  }

  advance()
  expression()
  consume(TOKEN.EOF, "Expect end of expression.")
  endCompiler()

  return {
    success: !parser.hadError,
    chunks: chunks,
  }
}

mod
