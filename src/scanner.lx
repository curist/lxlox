let lib = import "src/lib.lx"
let types = import "src/types.lx"
let TOKEN = types.TOKEN

fn initScanner(src) {
  let state = {
    start: 0,
    current: 0,
    line: 1,
  }

  let srcLen = len(src)
  fn isAtEnd() {
    return srcLen == state.current
  }

  fn getLexeme() {
    let lexeme = ""
    let i = state.start
    for i < state.current {
      lexeme = lexeme + src[i]
      i = i + 1
    }
    return lexeme
  }

  fn Token(type) {
    return {
      type: type,
      lexeme: getLexeme(),
      line: state.line,
    }
  }

  fn errorToken(message) {
    return {
      type: TOKEN.ERROR,
      lexeme: message,
      line: state.line,
    }
  }

  fn advance() {
    let current = state.current
    state.current = current + 1
    return src[current]
  }

  fn match(expected) {
    if isAtEnd() {
      return false
    }
    if src[state.current] != expected {
      return false
    }
    state.current = state.current + 1
    return true
  }

  fn peek() {
    return src[state.current]
  }

  fn peekNext() {
    if isAtEnd() { nil } else src[state.current + 1]
  }

  fn skipWhitespace() {
    for !isAtEnd() {
      let c = peek()
      if c == " " or c == "\r" or c == "\t" {
        advance()
      } else if c == "\n" {
        state.line = state.line + 1
        advance()
      } else if c == "/" {
        if peekNext() == "/" {
          for peek() != "\n" and !isAtEnd() { advance() }
        } else {
          return
        }
      } else {
        return
      }
    }
  }

  let tokenHandlers = {
    ["("]: fn() { Token(TOKEN.LEFT_PAREN) },
    [")"]: fn() { Token(TOKEN.RIGHT_PAREN) },
    ["{"]: fn() { Token(TOKEN.LEFT_BRACE) },
    ["}"]: fn() { Token(TOKEN.RIGHT_BRACE) },
    ["["]: fn() { Token(TOKEN.LEFT_BRACKET) },
    ["]"]: fn() { Token(TOKEN.RIGHT_BRACKET) },
    [";"]: fn() { Token(TOKEN.SEMICOLON) },
    [","]: fn() { Token(TOKEN.COMMA) },
    ["."]: fn() { Token(TOKEN.DOT) },
    ["-"]: fn() { Token(TOKEN.MINUS) },
    ["%"]: fn() { Token(TOKEN.MOD) },
    ["+"]: fn() { Token(TOKEN.PLUS) },
    ["/"]: fn() { Token(TOKEN.SLASH) },
    ["*"]: fn() { Token(TOKEN.STAR) },
    ["!"]: fn() { Token(if match("=") { TOKEN.BANG_EQUAL } else TOKEN.BANG) },
    ["="]: fn() { Token(if match("=") { TOKEN.EQUAL_EQUAL } else TOKEN.EQUAL) },
    ["<"]: fn() { Token(if match("=") { TOKEN.LESS_EQUAL } else TOKEN.LESS) },
    [">"]: fn() { Token(if match("=") { TOKEN.GREATER_EQUAL } else TOKEN.GREATER) },
    ["\""]: fn() {
      for peek() != "\"" and !isAtEnd() {
        if peek() == "\n" {
          state.line = state.line + 1
        }
        advance()
      }

      if isAtEnd() { return errorToken("Unterminated string.") }

      // closing quote.
      advance()
      return Token(TOKEN.STRING)
    },
  }

  fn isDigit(n) { n >= "0" and n <= "9" }
  fn isAlpha(c) {
    (c >= "a" and c <= "z") or (c >= "A" and c <= "Z") or c == "_"
  }

  fn number() {
    for isDigit(peek()) { advance() }
    if peek() == "." and isDigit(peekNext()) {
      // consume the "."
      advance()

      for isDigit(peek()) { advance() }
    }
    return Token(TOKEN.NUMBER)
  }

  fn identifier() {
    for isAlpha(peek()) or isDigit(peek()) { advance() }
    let lexeme = getLexeme()
    let isKeyword = types.KEYWORDS[lexeme]

    return Token(if isKeyword {
      TOKEN[Lx.toUpperCase(lexeme)]
    } else TOKEN.IDENTIFIER)
  }

  fn scanToken() {
    skipWhitespace()
    state.start = state.current

    if isAtEnd() {
      return Token(TOKEN.EOF)
    }

    let c = advance()

    if isAlpha(c) { return identifier() }
    if isDigit(c) { return number() }

    if let handler = tokenHandlers[c] {
      return handler()
    }

    return errorToken("Unexpected character: " + c)
  }

  return {
    scanToken: scanToken
  }
}

